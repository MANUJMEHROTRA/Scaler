{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Lecture Notes for session conducted on September 09 2022\n",
        "\n",
        "https://www.scaler.com/academy/mentee-dashboard/class/34698/session\n",
        "\n",
        "**Content**\n",
        "\n",
        "1.   TF2 and Keras code for Sequential API.\n",
        "2.   TF2 and Keras code for Functional API.\n",
        "3.   Dropout (Regularization through randomization).\n",
        "4.   CPU v/s GPU.\n",
        "5.   TensorBoard and Computational Graph.\n",
        "6.   Batch Normalization."
      ],
      "metadata": {
        "id": "TJZ-3vAUjCBO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### TF2 and Keras (Sequential API)"
      ],
      "metadata": {
        "id": "XUKPDk91vxvC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Activation, Dropout\n",
        "from tensorflow.keras.utils import to_categorical, plot_model\n",
        "from tensorflow.keras.datasets import mnist\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "BytD5UlYFd0A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(tf.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L3ZNtPSTFd3T",
        "outputId": "4fe03569-3f31-4bb8-e985-45c3f3bea5bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.8.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load mnist dataset\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NNjYKWwgFd6G",
        "outputId": "5eaa0abd-ba2a-43ff-a64c-403c4cf77d7e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n",
            "11501568/11490434 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Keras has some inbuilt datasets to work with."
      ],
      "metadata": {
        "id": "r0L5cG4YM19U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Question:*** Is there any classical ML model which can be used when number of classes is much larger, lets say 100?\n",
        "\n",
        "***Answer:***\n",
        "- Strategy 1 v/s rest can be used in Logistic Regression.\n",
        "- KNN can also be used if 100 classes are well separated.\n",
        "- Dimension reduction using UMAP and Tree based methods like Decision Tree, Gradient Boost Decision Tree, Random Forest can be used. UMAP is used as it is faster and preserves Global structure better.\n",
        "- Softmax Classifier can be used as well. Think of it as Classical ML model.\n",
        " <img src='https://drive.google.com/uc?id=1fLO9ZMPRIdcP_jMm28-E51nb05lyNj7y'>\n"
      ],
      "metadata": {
        "id": "01A28RCLlrtn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Question:*** If classes are too high, does a decision tree or RF perform well? or generally it performs better on binary classification?\n",
        "\n",
        "***Answer:***\n",
        "- For each class as long as you have good amount of data, it should be ok.\n",
        "- If classes are more, people go to Deep Learning models."
      ],
      "metadata": {
        "id": "BsGvxbeEN9Eb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(x_train.shape)\n",
        "print(y_train.shape)\n",
        "print(x_test.shape)\n",
        "print(y_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xrnkl1xXFd9H",
        "outputId": "3a1d770e-ae25-4534-cc98-ad230f755157"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(60000, 28, 28)\n",
            "(60000,)\n",
            "(10000, 28, 28)\n",
            "(10000,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, we have 60,000 images in training data each of size 28 by 28 pixels as inputs. For each image we have entry in $y_{train}$ which is an integer representing the number in the image.\n",
        "\n",
        "Similarly, we have 10,000 images in test set with its correponding labels in $y_{test}$"
      ],
      "metadata": {
        "id": "A5-OTs62OaYL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "idx=1234\n",
        "plt.imshow(x_train[idx], cmap='gray', interpolation='none')\n",
        "print(y_train[idx])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "OkYIh43LFjMi",
        "outputId": "0c4752cc-8c45-4930-f6d2-7772fad36d71"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Question:*** Can we pose a 100 class classification problem as regression? Why? or Why not?\n",
        "\n",
        "***Answer:***\n",
        "- Ordinality is important in Regression. If data has no ordinality, then we can't.\n",
        "- Scale is needed. For e.g. let say we have classes $C_1,C_2...C_{100}$ where $C_1 \\to White \\ (255)$ and $C_{100} \\to Black \\ (0)$. So a perfect $Grey$ would lie in between $(128)$.\n",
        "  <img src='https://drive.google.com/uc?id=1w8TqO24wiFZVcUtimgGRWdu5XbxVu-RB'>\n",
        "  "
      ],
      "metadata": {
        "id": "JOrF65dPKuXI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Question:*** From Entropy/gini minimization POV (getting \"pure nodes\" as quickly as possible),  what impact will having higher number of classes have?\n",
        "\n",
        "***Answer:***\n",
        "- Entropy is represented as $-\\sum p_i log(p_i)$.\n",
        "- Imagine we have $2$ classes with $P_1 = 0.5$ and $P_2 = 0.5$ as probabilities for each of them. Imagine we have $10$ classes with probabilities $P_1, P_2,....P_{10}$ equal to $0.1$ for each of them.\n",
        "- If we substitute these values, then we get values as $0.3010$ for $2$ classes and $1$ for $10$ classes.\n",
        "- This shows that if we have more classes, then entropy calculation certainly gets impacted. But here, we are interested in difference in entropy. So it does impact.\n",
        "  <img src='https://drive.google.com/uc?id=1d7IDkhbzLWLBqoWEpjjV6p7gTEUfj3km'>\n",
        "  "
      ],
      "metadata": {
        "id": "hBSQKUwyK05U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# number of class labels\n",
        "num_labels = len(np.unique(y_train))\n",
        "print(num_labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4O7ElNDTFjPZ",
        "outputId": "2430b3bc-008c-4186-eab1-5e50ae1de03d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# convert y_i's to one-hot vectors\n",
        "y_train = to_categorical(y_train)\n",
        "y_test = to_categorical(y_test)"
      ],
      "metadata": {
        "id": "wMAUypETFjS4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since, we want to use Categorical Cross Entropy for loss, we need to use One-hot encoding."
      ],
      "metadata": {
        "id": "67T74T_WSki3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Image: 2D matrix to 1D vector\n",
        "image_size = x_train.shape[1]\n",
        "input_size = image_size * image_size\n",
        "\n",
        "# 2D to 1D + Normalize (0-255)\n",
        "x_train = np.reshape(x_train, [-1, input_size])\n",
        "x_train = x_train.astype('float32') / 255\n",
        "x_test = np.reshape(x_test, [-1, input_size])\n",
        "x_test = x_test.astype('float32') / 255"
      ],
      "metadata": {
        "id": "1UNmh07UFpuj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- A 2-D image of 28 by 28 pixels, is converted to 1D by placing each row in 2D representation one after the other to form a 1D representation.\n",
        "  <img src='https://drive.google.com/uc?id=16l3Ptw15bBIkZfb0LEmwDltVl7SLGxIt'>\n"
      ],
      "metadata": {
        "id": "ScbVchQnT8FU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Till now, we have used standardization of data i.e. Mean-centering and variance scaling.\n",
        "- But for images, we use normalization i.e. min-max scaling.\n",
        "- Typically in images we know that $0 \\to Black$ and $255 \\to White$. If standardization was used, then we would have negative values that cannot be interpreted.\n",
        "- But if we use normalization, then we know that $0 \\to Black$ and $1 \\to White$ with values in between representing various shades of grey.\n",
        "- Hence,normalization is used.\n",
        "  <img src='https://drive.google.com/uc?id=1jnPINiPpUSrgSyCjw33hY3q021T58w9k'>\n",
        "  "
      ],
      "metadata": {
        "id": "nL7tMq8CK6i9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyper-params of NN\n",
        "batch_size = 128 # typically 2^n\n",
        "hidden_units = 256"
      ],
      "metadata": {
        "id": "_rpWxiXmFpxJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Batch-size is set to 128. Typically it is $2^n$. This will be discussed in GPU section.\n",
        "- Hidden layer will contain 256 Neurons."
      ],
      "metadata": {
        "id": "SNaHt7ZHUkir"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Question:*** For a colored image, will we still do min max scaling as the distinction might not e between black and white only?\n",
        "\n",
        "***Answer:***\n",
        "- Any color is represented as RGB (Red,Green,Blue) each having a value between $0-255$.\n",
        "- Even here, we do min-max scaling. $Black \\to 0,0,0$, $White \\to 1,1,1$, $Red \\to 1,0,0$ and so on.\n",
        "  <img src='https://drive.google.com/uc?id=1Ug79O2c8xHk3L-c3nzLd7vUiH-QNZcVw'>"
      ],
      "metadata": {
        "id": "PYfZGt30LB5l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Question:*** What does batch size of 128 signify? Here 128 is the number of data points that we pass correct?\n",
        "\n",
        "***Answer:***\n",
        "- Each epoch has multiple iterations. In each iteration, we have random subset which is 'batch size'.\n",
        "- It corresponds to how many points we are passing in one iteration."
      ],
      "metadata": {
        "id": "PQhsOOPiV0KJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sequential API\n",
        "\n",
        "# 3-layer MLP with ReLU\n",
        "model = Sequential()\n",
        "model.add(Dense(hidden_units, input_dim=input_size))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dense(hidden_units))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dense(num_labels))\n",
        "# this is the output for one-hot vector\n",
        "model.add(Activation('softmax'))\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vl7VMLCsFp3H",
        "outputId": "2f37f7e4-2311-4c32-fa0a-8a84e8330b71"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense (Dense)               (None, 256)               200960    \n",
            "                                                                 \n",
            " activation (Activation)     (None, 256)               0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 256)               65792     \n",
            "                                                                 \n",
            " activation_1 (Activation)   (None, 256)               0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 10)                2570      \n",
            "                                                                 \n",
            " activation_2 (Activation)   (None, 10)                0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 269,322\n",
            "Trainable params: 269,322\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Question:*** How are number of parameters computed in the above summary?\n",
        "\n",
        "***Answer:***\n",
        "- Input is of shape 784*Batch Size which is densely connected to 256 Neurons. So parameters are computed as $(784*256) \\ weights + 256 \\ for \\ biases = 200960$.\n",
        "  <img src='https://drive.google.com/uc?id=1lwU9IQrn4Yo_QBGQoU3E9JaX-s9Qtl_c'>"
      ],
      "metadata": {
        "id": "bR6CbrciWdRO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Question:*** Is there a rule of thumb we can follow to determine hidden_layers and neurons in each hidden_layers?\n",
        "\n",
        "***Answer:***\n",
        "- Here single input each of size $784*1$ is passed through Neural network setup to get an output that is of size $10*1$.\n",
        "- So, we want the interemediate layers that are reducing.\n",
        "- Why 256?. Well it could be anything and hence it's a hyper parameter.\n",
        "  <img src='https://drive.google.com/uc?id=18c1nDwiXSSZQxwTH6gXuejmAhIsgKU3X'>\n",
        "\n",
        "\n",
        "*Note:* As we increase layers, we will certainly overfit.\n"
      ],
      "metadata": {
        "id": "vCSxcwaxkN7t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "3zarxyP-Fp5v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have $60,000$ images in training set. So for $20$ epochs and $128$ batch-size, we have $60,000/12 \\approx 469$ images used in 1 iteration of epoch."
      ],
      "metadata": {
        "id": "JJBbQOYhXL-5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# train the network\n",
        "%%time\n",
        "model.fit(x_train, y_train, epochs=20, batch_size=batch_size)\n",
        "\n",
        "#60000/128 ~ 469"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Od_qFBfnFp80",
        "outputId": "bf719ee8-a6d6-4ebd-d8f7-3c8874dd8923"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "469/469 [==============================] - 4s 7ms/step - loss: 0.2583 - accuracy: 0.9250\n",
            "Epoch 2/20\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.0967 - accuracy: 0.9708\n",
            "Epoch 3/20\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.0624 - accuracy: 0.9800\n",
            "Epoch 4/20\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.0458 - accuracy: 0.9858\n",
            "Epoch 5/20\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.0332 - accuracy: 0.9893\n",
            "Epoch 6/20\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.0273 - accuracy: 0.9909\n",
            "Epoch 7/20\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.0201 - accuracy: 0.9940\n",
            "Epoch 8/20\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.0165 - accuracy: 0.9947\n",
            "Epoch 9/20\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.0171 - accuracy: 0.9943\n",
            "Epoch 10/20\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.0159 - accuracy: 0.9945\n",
            "Epoch 11/20\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.0126 - accuracy: 0.9955\n",
            "Epoch 12/20\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.0106 - accuracy: 0.9965\n",
            "Epoch 13/20\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.0103 - accuracy: 0.9964\n",
            "Epoch 14/20\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.0113 - accuracy: 0.9962\n",
            "Epoch 15/20\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.0082 - accuracy: 0.9970\n",
            "Epoch 16/20\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.0083 - accuracy: 0.9971\n",
            "Epoch 17/20\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.0117 - accuracy: 0.9962\n",
            "Epoch 18/20\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.0062 - accuracy: 0.9981\n",
            "Epoch 19/20\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.0100 - accuracy: 0.9970\n",
            "Epoch 20/20\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.0066 - accuracy: 0.9977\n",
            "CPU times: user 1min 33s, sys: 6.03 s, total: 1min 39s\n",
            "Wall time: 1min 7s\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fe66e542c50>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Note:*\n",
        "%%time in Python computes the execution time."
      ],
      "metadata": {
        "id": "5i71yz0GXp0M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# test dataset  performance\n",
        "loss, acc = model.evaluate(x_test,\n",
        "                        y_test,\n",
        "                        batch_size=batch_size,\n",
        "                        verbose=0)\n",
        "print(\"\\nTest accuracy: %.1f%%\" % (100.0 * acc))\n",
        "print(loss)\n",
        "\n",
        "# Overfit or Underfit?"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BLlTrPvHFp_s",
        "outputId": "24fc9ebc-9f9c-4fd7-bad9-4044cf688198"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test accuracy: 97.9%\n",
            "0.10156406462192535\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Question:*** Do you think the model has performed well on test set?\n",
        "\n",
        "***Answer:***\n",
        "- Training loss is $0.0066$ with accuracy as $99.7\\%$, whereas test loss is $0.10$ with accuracy as $97.9\\%$\n",
        "- Even though test loss is very small the difference between train loss and test loss is quite significant.\n",
        "- Accuracy is just a business matrix. We are optimizing for log-loss.\n",
        "  <img src='https://drive.google.com/uc?id=18i_fuxSqEcF2ZcMHknDlohbo674MjgrV'>\n",
        "\n",
        "\n",
        "*Note:* Screenshot values may not match with values we get when running the code as samples are drawn randomly."
      ],
      "metadata": {
        "id": "4Cqj70FwLTNu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Question:*** Is one iteration / step is one forward and then one back propagation?\n",
        "\n",
        "***Answer:***\n",
        "- Yes, that is correct."
      ],
      "metadata": {
        "id": "dOVOVW62mjRE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### TF 2 and Keras: Functional API"
      ],
      "metadata": {
        "id": "Yz9sfksvFzfU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(x_train.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z6iBNc_uFqCa",
        "outputId": "610017a8-3e6e-43bf-e093-b19bc481e053"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(60000, 784)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = keras.Input(shape=(784,))\n",
        "inputs.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "beIB6WiQF3Gh",
        "outputId": "6b78c350-2165-42b0-d3f1-4e60c8d23334"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([None, 784])"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Functional API: much more intuitive\n",
        "# Refer: https://www.tensorflow.org/guide/keras/functional\n",
        "x1 = layers.Dense(hidden_units, activation=\"relu\")(inputs)\n",
        "x2 = layers.Dense(hidden_units, activation=\"relu\")(x1)\n",
        "outputs = layers.Dense(num_labels, activation=\"softmax\")(x2)\n",
        "\n",
        "model = keras.Model(inputs=inputs, outputs=outputs, name=\"simple_model\")\n",
        "model.summary()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D6dlKynpF3Kt",
        "outputId": "41a59818-2baa-4679-b39a-9a85c1bb1982"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"simple_model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 784)]             0         \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 256)               200960    \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 256)               65792     \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 10)                2570      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 269,322\n",
            "Trainable params: 269,322\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Sequential API is good when we have sequential layers.\n",
        "- But imagine if we have complex multi-input setup as follows with non-sequential connections.\n",
        "  <img src='https://drive.google.com/uc?id=1bILPP5WcYYEHWOuKtQmvIwgpvmCRCadB'>\n",
        "- In such scenarios Functional APIs are easy to use.\n",
        "\n",
        "\n",
        "\n",
        "  "
      ],
      "metadata": {
        "id": "Vt_PwvlzLaD-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- We can plot the architecture of Neural Network setup using Functional API as follows:\n",
        "\n",
        "  <img src='https://drive.google.com/uc?id=1qWubyCtVZTWoe5-NTAak8yl0Ps6OTRmO'>\n",
        "\n",
        "  <img src='https://drive.google.com/uc?id=10JadG1bO6bwZjXeG0kZthjpUnC2wA8y9'>"
      ],
      "metadata": {
        "id": "XRFU3_QNnVgy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Question:*** Why do we only use linear weighted layers being passed through non linear activation functions? Can we also use lets say tree based network for any layers?\n",
        "\n",
        "***Answer:***\n",
        "- People have designed tree based networks but those haven't out performed.\n",
        "- Matrix multiplication in DL can be done much faster using modern gaming hardware."
      ],
      "metadata": {
        "id": "hNL4nR7VoVcl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- While using functional API, input of size (784,) $(inputs)$ is passed to dense layer with 256 neurons using ReLu activations function for each of the neuron $x_1$.\n",
        "- $x_1$ is further passed to a dense layer with 256 neurons using ReLu activations function for each of the neuron $x_2$.\n",
        "- $x_2$ is passed to a dense layers with 10 neurons using Softmax layer which generates output.\n",
        "  <img src='https://drive.google.com/uc?id=1dhrTYnP9MCT3pf2orpYp2r9Dmo8y-_TT'>\n"
      ],
      "metadata": {
        "id": "KtWeQE5WLxRA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "ltYhvEZ5F3Nn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train the network\n",
        "%%time\n",
        "model.fit(x_train, y_train, epochs=20, batch_size=batch_size)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FHrUUKrzF3Qa",
        "outputId": "dbe9a552-6bc6-4f5b-fcdb-6cea6a97244c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "469/469 [==============================] - 4s 7ms/step - loss: 0.2543 - accuracy: 0.9259\n",
            "Epoch 2/20\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.0951 - accuracy: 0.9713\n",
            "Epoch 3/20\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.0623 - accuracy: 0.9805\n",
            "Epoch 4/20\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.0457 - accuracy: 0.9855\n",
            "Epoch 5/20\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.0337 - accuracy: 0.9891\n",
            "Epoch 6/20\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.0268 - accuracy: 0.9914\n",
            "Epoch 7/20\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.0212 - accuracy: 0.9931\n",
            "Epoch 8/20\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.0166 - accuracy: 0.9947\n",
            "Epoch 9/20\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.0186 - accuracy: 0.9937\n",
            "Epoch 10/20\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.0111 - accuracy: 0.9963\n",
            "Epoch 11/20\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.0125 - accuracy: 0.9956\n",
            "Epoch 12/20\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.0134 - accuracy: 0.9954\n",
            "Epoch 13/20\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.0086 - accuracy: 0.9971\n",
            "Epoch 14/20\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.0094 - accuracy: 0.9969\n",
            "Epoch 15/20\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.0047 - accuracy: 0.9984\n",
            "Epoch 16/20\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.0110 - accuracy: 0.9965\n",
            "Epoch 17/20\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.0103 - accuracy: 0.9964\n",
            "Epoch 18/20\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.0101 - accuracy: 0.9967\n",
            "Epoch 19/20\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.0083 - accuracy: 0.9973\n",
            "Epoch 20/20\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.0058 - accuracy: 0.9983\n",
            "CPU times: user 1min 32s, sys: 5.6 s, total: 1min 38s\n",
            "Wall time: 1min 5s\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fe66a1cf3d0>"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Summary of API:"
      ],
      "metadata": {
        "id": "GqfECygWrqm9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are 2 ways to design Deep Learning Architecture:\n",
        "- Sequential API.\n",
        "- Functional API.\n",
        "\n",
        "For non-sequential model, Functional API is useful."
      ],
      "metadata": {
        "id": "t3aNp52Fr3B6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Dropout: Regularization through randomization"
      ],
      "metadata": {
        "id": "91vd6G-hGFlq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Intuition:"
      ],
      "metadata": {
        "id": "9mSqd5UDtD1X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- In Random forests, base learners are deep trees i.e. they are high variance models. Trick used here is row-sampling or column-sampling.\n",
        "- We take average of each base learners using majority voting. So in Random forests, we used randomization for regularization.\n",
        "- Similar strategy is used in Dropouts."
      ],
      "metadata": {
        "id": "_7aM21vzsRGi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Trivia:*** Dropout was a Masters Thesis published by Nitish Srivastava.\n",
        "\n",
        "https://www.cs.toronto.edu/~rsalakhu/papers/srivastava14a.pdf"
      ],
      "metadata": {
        "id": "Ls0WmBKetzx0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Imagine we have a Fully connected Neural Network for binary class classification problem.\n",
        "- Dropout basically says when we pass a batch of inputs to the model, we randomly drop/ignore some edges or the neurons itself.\n",
        "- What it means that we won't update weights and biases for some of the neurons.\n",
        "- Neurons are randomly selected using $dropout \\ rate$. For example let say, dropout rate = $20\\%$. Then it is across entire network and not just single layer.\n",
        "  <img src='https://drive.google.com/uc?id=14EcbXOS5WyngmfHdn_qNrcI9N40tLg0H'>"
      ],
      "metadata": {
        "id": "qoSe0auHL4PY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Implementation:"
      ],
      "metadata": {
        "id": "d7A9asxTu-9g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- For every neuron, get a random number between $0 \\to 1$.\n",
        "- Let  dropout rate is $20\\%$.\n",
        "- If for a neuron the randomly generated number is $\\le 0.2$, then ignore that neuron.\n",
        "\n",
        "  <img src='https://drive.google.com/uc?id=1Lw02ZYg-h7FZpmtrNJRfb8CsDeguLere'>"
      ],
      "metadata": {
        "id": "88xuusqsMAMo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Here, we are randomly selecting neurons to drop-off.\n",
        "- Imagine we have dropout = $20\\%$. Let say we have $20$ epochs with 500 iterations in each epoch. In total, we have $10,000$ iterations.\n",
        "- Of these $10,000$ iterations, a neuron did not contribute to network for $2000$ iterations.\n",
        "- So everytime, we trained only $80\\%$ of neurons.\n",
        "  <img src='https://drive.google.com/uc?id=14K2fwR3TdJ2wg7bCdofSFIQTvUN0UDiD'>\n",
        "- When we have more layers, then we have computationally more weights and biases to learn.\n",
        "- More neurons means we have more computational power of network and we can learn a more complex thing.\n",
        "- Here we use Dropout, if we don't want to overfit.\n"
      ],
      "metadata": {
        "id": "k1U6N3nAMFpo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Question:*** Is dropout only for computional reduce or any significant change in results?\n",
        "\n",
        "***Answer:***\n",
        "- No. It is done to regularize model, so that it doesn't overfit."
      ],
      "metadata": {
        "id": "EC7y5GtCxd97"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Question:*** Rather than this, why not have lesser neurons in the hidden layer? It will not overfit right?\n",
        "\n",
        "***Answer:***\n",
        "- Moment we reduce neurons, model's capacity to learn has reduced.\n",
        "- Ideally we want model to capture non-linearity without overfitting."
      ],
      "metadata": {
        "id": "mPWocODCxNAL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2 approaches for Dropout:"
      ],
      "metadata": {
        "id": "vxDeZFVWyRcg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- We either dropoff a neuron or we drop an edge itself.\n",
        "\n",
        "*Note:* In TF2, the edge is dropped-off."
      ],
      "metadata": {
        "id": "duR7UqwoyX30"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Question:*** Turning off of a particular neuron happens for the whole epoch, or it changes in every iteration of an epoch?\n",
        "\n",
        "***Answer:***\n",
        "- This is more of an engineering choice rather than Scientific one.\n",
        "- Ther are some libraries that implement it for an iteration, while some that do it for an epoch."
      ],
      "metadata": {
        "id": "JOzAPgJDx3gd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Important aspects of Dropout:"
      ],
      "metadata": {
        "id": "h7ATGCPHyzkB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Training:"
      ],
      "metadata": {
        "id": "aj3DRLt2y8MB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Imagine we multiple neurons and have implemented edge level dropouts.\n",
        "- Let there be 4 inputs $i_1, i_2,i_3,i_4$ as inputs to a neuron of which edge from $i_1$ is dropped out.\n",
        "- In Forward propogation, all the inputs will flow but edge from $i_1$ is zeroed out.\n",
        "- Output of the neuron is computed as $ReLu(0+i_2*w_2+i_3*w_3+i_4*w_4)$.\n",
        "- In back propogation, derivative of this output is computed as $\\frac{\\partial L}{\\partial O_p}$. Here we don't calculate partial derivative wrt $w_1$ and we only update $w_2,w_3,w_4$.\n",
        "  <img src='https://drive.google.com/uc?id=10vcSJnlnGl4wluZHJ3-u71EruQOmulZO'>\n",
        "  "
      ],
      "metadata": {
        "id": "EbtptC5SMTUH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Test Time:"
      ],
      "metadata": {
        "id": "0RlKPH19549Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- We multiply weights with probability '$p$'. Let say, the edge was present $80\\%$ of times. At test time, $w^k_{ij}$ is computed as $(1-r)*w^k_{ij}$, where $r$ is dropout rate.\n",
        "  <img src='https://drive.google.com/uc?id=1piDD9RVfxEJh8PiTRyclRSxeAUzh98fA'>\n",
        "- Doing so, we do not skip any edge.\n",
        "  <img src='https://drive.google.com/uc?id=1P_6kORU9wuC9jrBEskMnK-cQDJ9EOUfT'>\n",
        "\n",
        "\n",
        "  "
      ],
      "metadata": {
        "id": "-q3gyPj_549z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Question:*** If we use ReLu, then also there is possibility of some input to be 0. Does it makes things worse for Dropout, or it helps?\n",
        "\n",
        "***Answer:***\n",
        "- No, it does not impact.\n",
        "  <img src='https://drive.google.com/uc?id=1_fcR8XzJMBpodRXWyWSncE95AG7tG7eB'>\n"
      ],
      "metadata": {
        "id": "Kh9XKXcw69hV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Question:*** But the chain rule breaks if we ignore that edge. We might have used the weight from its previous chain?\n",
        "\n",
        "***Answer:***\n",
        "- No, it does not.\n",
        "- For e.g. in the below network setup, let say we drop the edge from $f_{11}$ to $f_{21}$.\n",
        "- Partial derivative of Loss ($L$) wrt to $w_1$ is computed as:\n",
        "$$\\frac{\\partial L}{\\partial w_1} = \\frac{\\partial L}{\\partial O_{31}} * \\frac{\\partial O_{31}}{\\partial O_{22}} * \\frac{\\partial O_{22}}{\\partial O_{11}} * \\frac{\\partial O_{11}}{\\partial w_{1}}$$\n",
        "  <img src='https://drive.google.com/uc?id=1j4jnR1mUu1WtZh-K8NWPCIvmeDF2FTWS'>\n"
      ],
      "metadata": {
        "id": "3RCE0SHd7fCm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Performance Comparison:"
      ],
      "metadata": {
        "id": "3ZDkw579BYy0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- In the below graph, we are comparing 4 different architectures with and without dropout.\n",
        "- Without dropout, error keeps oscillating without much improvement.\n",
        "- But with dropout, there is a significant reduction in error.  \n",
        "  <img src='https://drive.google.com/uc?id=1QkFF5wiIHZZ76c96GiXuKmzmZFw95VG2'>\n",
        "- Notice, there is a reduction in error even if there are more iterations."
      ],
      "metadata": {
        "id": "o110wU7aBTDP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Dropout Code:"
      ],
      "metadata": {
        "id": "I8PIRSfqChMb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model with Dropout\n",
        "x = layers.Dense(hidden_units, activation=\"relu\")(inputs)\n",
        "x = Dropout(0.3)(x)\n",
        "x = layers.Dense(hidden_units, activation=\"relu\")(x)\n",
        "x = Dropout(0.3)(x)\n",
        "outputs = layers.Dense(num_labels, activation=\"softmax\")(x)\n",
        "\n",
        "model = keras.Model(inputs=inputs, outputs=outputs, name=\"simple_model\")\n",
        "model.summary()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zyq5bpxwCkA1",
        "outputId": "0855568c-5af5-463e-ad9e-27c7ac0a1e26"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"simple_model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 784)]             0         \n",
            "                                                                 \n",
            " dense_6 (Dense)             (None, 256)               200960    \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 256)               0         \n",
            "                                                                 \n",
            " dense_7 (Dense)             (None, 256)               65792     \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 256)               0         \n",
            "                                                                 \n",
            " dense_8 (Dense)             (None, 10)                2570      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 269,322\n",
            "Trainable params: 269,322\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "VlH08qCZ5490"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# Increased epochs as we added dropouts\n",
        "model.fit(x_train, y_train, epochs=30, batch_size=batch_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e4e6cab6-b415-4b4f-900e-4cba65ddc980",
        "id": "QwYu3bw35490"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "469/469 [==============================] - 4s 8ms/step - loss: 0.3441 - accuracy: 0.8970\n",
            "Epoch 2/30\n",
            "469/469 [==============================] - 4s 8ms/step - loss: 0.1491 - accuracy: 0.9553\n",
            "Epoch 3/30\n",
            "469/469 [==============================] - 4s 8ms/step - loss: 0.1161 - accuracy: 0.9643\n",
            "Epoch 4/30\n",
            "469/469 [==============================] - 4s 8ms/step - loss: 0.0934 - accuracy: 0.9707\n",
            "Epoch 5/30\n",
            "469/469 [==============================] - 4s 8ms/step - loss: 0.0793 - accuracy: 0.9749\n",
            "Epoch 6/30\n",
            "469/469 [==============================] - 4s 8ms/step - loss: 0.0705 - accuracy: 0.9774\n",
            "Epoch 7/30\n",
            "469/469 [==============================] - 4s 8ms/step - loss: 0.0634 - accuracy: 0.9801\n",
            "Epoch 8/30\n",
            "469/469 [==============================] - 4s 8ms/step - loss: 0.0562 - accuracy: 0.9824\n",
            "Epoch 9/30\n",
            "469/469 [==============================] - 4s 8ms/step - loss: 0.0533 - accuracy: 0.9829\n",
            "Epoch 10/30\n",
            "469/469 [==============================] - 4s 8ms/step - loss: 0.0477 - accuracy: 0.9842\n",
            "Epoch 11/30\n",
            "469/469 [==============================] - 4s 8ms/step - loss: 0.0437 - accuracy: 0.9851\n",
            "Epoch 12/30\n",
            "469/469 [==============================] - 4s 8ms/step - loss: 0.0421 - accuracy: 0.9860\n",
            "Epoch 13/30\n",
            "469/469 [==============================] - 4s 8ms/step - loss: 0.0385 - accuracy: 0.9873\n",
            "Epoch 14/30\n",
            "469/469 [==============================] - 4s 8ms/step - loss: 0.0365 - accuracy: 0.9878\n",
            "Epoch 15/30\n",
            "469/469 [==============================] - 4s 8ms/step - loss: 0.0358 - accuracy: 0.9876\n",
            "Epoch 16/30\n",
            "469/469 [==============================] - 4s 8ms/step - loss: 0.0348 - accuracy: 0.9888\n",
            "Epoch 17/30\n",
            "469/469 [==============================] - 4s 8ms/step - loss: 0.0349 - accuracy: 0.9884\n",
            "Epoch 18/30\n",
            "469/469 [==============================] - 4s 8ms/step - loss: 0.0313 - accuracy: 0.9897\n",
            "Epoch 19/30\n",
            "469/469 [==============================] - 4s 8ms/step - loss: 0.0281 - accuracy: 0.9912\n",
            "Epoch 20/30\n",
            "469/469 [==============================] - 4s 8ms/step - loss: 0.0284 - accuracy: 0.9904\n",
            "Epoch 21/30\n",
            "469/469 [==============================] - 4s 8ms/step - loss: 0.0272 - accuracy: 0.9907\n",
            "Epoch 22/30\n",
            "469/469 [==============================] - 4s 8ms/step - loss: 0.0265 - accuracy: 0.9907\n",
            "Epoch 23/30\n",
            "469/469 [==============================] - 4s 8ms/step - loss: 0.0254 - accuracy: 0.9915\n",
            "Epoch 24/30\n",
            "469/469 [==============================] - 4s 8ms/step - loss: 0.0245 - accuracy: 0.9918\n",
            "Epoch 25/30\n",
            "469/469 [==============================] - 4s 8ms/step - loss: 0.0265 - accuracy: 0.9909\n",
            "Epoch 26/30\n",
            "469/469 [==============================] - 4s 8ms/step - loss: 0.0254 - accuracy: 0.9914\n",
            "Epoch 27/30\n",
            "469/469 [==============================] - 4s 8ms/step - loss: 0.0233 - accuracy: 0.9924\n",
            "Epoch 28/30\n",
            "469/469 [==============================] - 4s 8ms/step - loss: 0.0234 - accuracy: 0.9927\n",
            "Epoch 29/30\n",
            "469/469 [==============================] - 4s 8ms/step - loss: 0.0234 - accuracy: 0.9920\n",
            "Epoch 30/30\n",
            "469/469 [==============================] - 4s 8ms/step - loss: 0.0220 - accuracy: 0.9924\n",
            "CPU times: user 2min 42s, sys: 9.23 s, total: 2min 51s\n",
            "Wall time: 1min 53s\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fe66a095690>"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# test dataset  performance\n",
        "loss, acc = model.evaluate(x_test,\n",
        "                        y_test,\n",
        "                        batch_size=batch_size,\n",
        "                        verbose=0)\n",
        "print(\"\\nTest accuracy: %.1f%%\" % (100.0 * acc))\n",
        "print(loss)\n",
        "\n",
        "# Overfit or Underfit?\n",
        "# Can we add more layers?"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fdaebcae-b5fc-4161-82d3-9ccd2382b374",
        "id": "ZpLVue4q5490"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test accuracy: 98.4%\n",
            "0.07547812163829803\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training Loss: $0.02$ with Accuracy: $99.24\\%$ Test Loss: $0.07$ with Accuracy: $98.4\\%$\n",
        "\n",
        "Even though there is a difference between loss, it is much smaller in comparison to setup without dropout.\n",
        "\n"
      ],
      "metadata": {
        "id": "zB4RTS41DObz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Question:*** If the edge between an input and next layer is dropped off, what value is assigned to its weight?\n",
        "\n",
        "***Answer:***\n",
        "- Imagine we have a setup like below. The edge $w^1_{11}$ is randomly initialized to a value initially (Let say $0.1$)\n",
        "- In iteration 1, if the edge is dropped off, then it is not updated.\n",
        "- In the next iteration, $w^1_{11}$ is updated as $w^1_{11} = 0.1 - \\eta* \\frac{\\partial L}{\\partial w^1_{11}}$.\n",
        "  <img src='https://drive.google.com/uc?id=1kNs5zxvWYZZLwMtDFjChCeheBQXjFGwn'>\n",
        "  "
      ],
      "metadata": {
        "id": "95Ph3XNdD5hc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Question:***\n",
        "1. Why is logistic regression considered as a linear classifier ? Is it only bcoz the underlying separator is a hyper plane ? The output is derived from the sigmoid function which is non linear in nature. So why is it not considered as a non linear separator?\n",
        "\n",
        "2. Perceptron can only separate linear data. But as we have learnt that the activation functions are non linear. Then why can it not classify non linear data ?\n",
        "\n",
        "***Answer:***\n",
        "- Yes, Decision boundary is linear separation. We apply Sigma function to get probabilities between 0 and 1.\n",
        "- Perceptron is basically like applying step function.\n",
        "- In logistic regression,  represents how far away is the point from hyperplane and on which side does it it lie.\n",
        "  <img src='https://drive.google.com/uc?id=1RB_STGn1JQgPCAtfrG8XgIqNOlNBbqOG'>\n",
        "  \n",
        "\n",
        "  "
      ],
      "metadata": {
        "id": "hbhwNIvlE7Bn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Question:*** What is derivative of ReLu function $ReLu()$, where $=3*x+5$?\n",
        "\n",
        "***Answer:***\n",
        "- $ReLu()$ is defined as $max(0,)$.\n",
        "- It's derivative is $+1$ for $x>0$, undefined at $0$ and $0$ when $x<0$.\n",
        "- Derivative is computed as:\n",
        "$$\\frac{\\partial ReLu(3x+5)}{\\partial x} = \\frac{\\partial ReLu()}{\\partial } * \\frac{\\partial (3x+5)}{\\partial x}$$\n",
        "- This comes out to be 3.\n",
        "\n",
        "  <img src='https://drive.google.com/uc?id=1uinQf7h6OoJAwfLaFa7VeaBdChCw8aFJ'>\n",
        "  \n"
      ],
      "metadata": {
        "id": "D5P66cd3Hutk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Question:*** If we have 2 datasets of same dimensions(same N, d). The first one has two classes (no imbalance). The other one has, say, 20 classes(no imbalance). If we are training using decision trees, will we get \"pure nodes\" more quickly on the second dataset(multiclass) compared to the first(binary) one?\n",
        "\n",
        "***Answer:***\n",
        "- Not necessary, it all depends upon how well separated the classes are. If they are not then getting pure nodes will take time.\n",
        "  <img src='https://drive.google.com/uc?id=1rl8AB3RzoCY7Ucd9NTSkgkaABm8nZbkC'>\n",
        "  \n",
        "\n"
      ],
      "metadata": {
        "id": "A9WODT0FIzg6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Note:"
      ],
      "metadata": {
        "id": "ydU5aeLLEiev"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Following topics will be covered in next the class:\n",
        "\n",
        "-   CPU v/s GPU.\n",
        "-   TensorBoard and Computational Graph.\n",
        "-   Batch Normalization."
      ],
      "metadata": {
        "id": "Pc9jhYArD06D"
      }
    }
  ]
}